{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce568af-4dfb-4637-80f1-792bc0973951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fd94c-94c6-487d-9187-8141c313e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import texts for analysis\n",
    "for name in file_names:\n",
    "    globals()[name] = sc.textFile(f\"/FileStore/tables/{name}.txt\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3ea81-cd7f-4af7-8731-68381a508f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#moving texts into dictionary\n",
    "texts_dict = {name: globals()[name] for name in file_names}\n",
    "print(texts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5156aa-f482-444e-a32b-fc532130ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the dictionary into a DataFrame\n",
    "texts_dict_df = {}\n",
    "\n",
    "for name, text_list in texts_dict.items():\n",
    "    text_rdd = sc.parallelize(text_list)\n",
    "    text_df = text_rdd.map(lambda x: (x, )).toDF([\"text\"])\n",
    "    texts_dict_df[f\"{name}_df\"] = text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f5e7a-bf88-435e-8d65-b7898c984180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning, filtering, and tokenizing texts\n",
    "texts_cleaned = {}\n",
    "\n",
    "for key, df in texts_dict_df.items():\n",
    "    cleaned_df = df.withColumn(\"cleaned_text\", \n",
    "                               regexp_replace(col(\"text\"), r\"[^\\w\\sąćęłńóśźżĄĆĘŁŃÓŚŹŻ]\", \"\"))\n",
    "    tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "    tokenized_cleaned_df = tokenizer.transform(cleaned_df)\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "    filtered_cleaned_df = remover.transform(tokenized_cleaned_df)\n",
    "    texts_cleaned[key + \"_cleaned\"] = filtered_cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd11ac0-c060-4ab0-a3ab-068d5e41af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting text into individual words\n",
    "texts_exploded = {}\n",
    "\n",
    "for key, df in texts_cleaned.items():\n",
    "    df.printSchema()\n",
    "    exploded_df = df.select(explode(df[\"filtered_words\"]).alias(\"word\"))\n",
    "    texts_exploded[key + \"_exploded\"] = exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e9bc89-e9ba-4626-b37d-10776b9ef7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the PoliMorf dictionary\n",
    "file_path = \"dbfs:/FileStore/tables/PoliMorf_0_6_7.tab\"\n",
    "morph_df_spark = spark.read.text(file_path)\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "morph_df_spark = morph_df_spark.withColumn(\"split_data\", split(morph_df_spark[\"value\"], \"\\t\"))\n",
    "morph_df_spark = morph_df_spark.select(\n",
    "    morph_df_spark[\"split_data\"][0].alias(\"form\"),\n",
    "    morph_df_spark[\"split_data\"][1].alias(\"lemma\"),\n",
    "    morph_df_spark[\"split_data\"][2].alias(\"part_of_speech\"),\n",
    "    morph_df_spark[\"split_data\"][3].alias(\"type\")\n",
    ")\n",
    "morph_df_spark.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092ca84-fe57-4799-93ec-aa4c5a653339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the affective dictionary\n",
    "csv_file_path = \"dbfs:/FileStore/tables/slownik.csv\"\n",
    "\n",
    "nawl_spark_df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"encoding\", \"windows-1250\").csv(csv_file_path)\n",
    "\n",
    "nawl_spark_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ccfe5-e25c-4d92-b121-a153027ad5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining texts with the morphological dictionary\n",
    "texts_joined = {}\n",
    "\n",
    "for key, exploded_df in texts_exploded.items():\n",
    "    joined_df = exploded_df.join(\n",
    "        morph_df_spark,\n",
    "        exploded_df.word == morph_df_spark.form,  \n",
    "        \"left\"  \n",
    "    )\n",
    "    texts_joined[key + \"_joined\"] = joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e4b17-b8f2-4d6c-98ea-8ca00f425750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out redundant matches from the morphological dictionary\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "texts_first_lemma = {}\n",
    "\n",
    "for key, joined_df in texts_joined.items():\n",
    "    window_spec = Window.partitionBy(\"word\").orderBy(\"form\")\n",
    "    first_lemma_df = joined_df.withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n",
    "                              .filter(\"row_num = 1\") \\\n",
    "                              .drop(\"row_num\")\n",
    "    texts_first_lemma[key + \"_first_lemma\"] = first_lemma_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45852fa7-f0a8-4a3d-bbbb-ed28fc77cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming column to avoid name conflicts later\n",
    "for key, unique_lemmas_df in texts_first_lemma.items():\n",
    "    unique_lemmas_df = unique_lemmas_df.withColumnRenamed(\"word\", \"lemma_word\")\n",
    "    texts_first_lemma[key] = unique_lemmas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8da23-c459-42e8-bf98-2e45f7a01e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the affective dictionary to the data frame\n",
    "texts_final = {}\n",
    "for key, first_lemma_df in texts_first_lemma.items():\n",
    "    final_df = first_lemma_df.join(nawl_spark_df, first_lemma_df.lemma == nawl_spark_df.word, \"left\")\n",
    "    texts_final[key + \"_final\"] = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7604b1-6f3e-4420-abb1-ab5ec4889e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizing emotions for the set of texts\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def calculate_emotions_summary(final_df):\n",
    "    aggregated_emotions = final_df.select(\n",
    "        F.avg(\"mean Happiness\").alias(\"avg_Happiness\"),\n",
    "        F.avg(\"mean Anger\").alias(\"avg_Anger\"),\n",
    "        F.avg(\"mean Sadness\").alias(\"avg_Sadness\"),\n",
    "        F.avg(\"mean Fear\").alias(\"avg_Fear\"),\n",
    "        F.avg(\"mean Disgust\").alias(\"avg_Disgust\")\n",
    "    ).first()  \n",
    "\n",
    "    emotions = {\n",
    "        \"Happiness\": aggregated_emotions[\"avg_Happiness\"],\n",
    "        \"Anger\": aggregated_emotions[\"avg_Anger\"],\n",
    "        \"Sadness\": aggregated_emotions[\"avg_Sadness\"],\n",
    "        \"Fear\": aggregated_emotions[\"avg_Fear\"],\n",
    "        \"Disgust\": aggregated_emotions[\"avg_Disgust\"]\n",
    "    }\n",
    "\n",
    "    filtered_emotions = {emotion: value for emotion, value in emotions.items() if value is not None}\n",
    "\n",
    "    dominant_emotion = max(filtered_emotions, key=filtered_emotions.get) if filtered_emotions else None\n",
    "\n",
    "    return {\n",
    "        \"dominant_emotion\": dominant_emotion,\n",
    "        \"emotion_values\": filtered_emotions\n",
    "    }\n",
    "\n",
    "emotions_summary = {}\n",
    "\n",
    "for key, final_df in texts_final.items():\n",
    "    final_df.cache()\n",
    "    emotion_summary = calculate_emotions_summary(final_df)\n",
    "    emotions_summary[key] = emotion_summary\n",
    "    print(f\"Emotion summary for document {key}:\")\n",
    "    print(f\"Dominant emotion: {emotion_summary['dominant_emotion']}\")\n",
    "    print(f\"Average emotion values: {emotion_summary['emotion_values']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75646b92-2060-4b45-be58-8fce539a988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming variables (for convenience)\n",
    "emotions_summary_renamed = {}\n",
    "\n",
    "old_fragment = \"df_cleaned_exploded_joined_first_lemma_final\"\n",
    "new_fragment = \"emotions\"\n",
    "\n",
    "for key, value in emotions_summary.items():\n",
    "    new_key = key.replace(old_fragment, new_fragment)\n",
    "    emotions_summary_renamed[new_key] = value\n",
    "\n",
    "emotions_summary_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5995d2-84eb-4043-b9ca-a5e858123685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transferring emotions from dictionary to data frame\n",
    "import pandas as pd\n",
    "emotion_data = []\n",
    "\n",
    "for doc, summary in emotions_summary_renamed.items():\n",
    "    emotion_values = summary['emotion_values']\n",
    "    emotion_data.append({\n",
    "        'Document': doc,\n",
    "        'Happiness': emotion_values['Happiness'],\n",
    "        'Anger': emotion_values['Anger'],\n",
    "        'Sadness': emotion_values['Sadness'],\n",
    "        'Fear': emotion_values['Fear'],\n",
    "        'Disgust': emotion_values['Disgust']\n",
    "    })\n",
    "\n",
    "emotion_df = pd.DataFrame(emotion_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c2279-058d-4237-ad35-d97cc1022373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing emotion values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = emotion_df.drop(columns=['Document'])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00781bf2-8bb3-4869-b912-5640d83fd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means cluster analysis using the Elbow method\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inertias = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), inertias)\n",
    "plt.xlabel('Liczba klastrów (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Metoda Elbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d758b6c-df69-4cdb-8784-b147edb429fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means clustering for the selected number of clusters\n",
    "k = 3  \n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "emotion_df['Cluster'] = kmeans.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32cb9d-d0b1-480b-8775-27b80b7cc905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview of clustering results\n",
    "print(emotion_df[['Document', 'Cluster']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e15487-6349-4fe2-8203-8426f2e46303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# characteristics of individual clusters\n",
    "cluster_means = emotion_df.groupby('Cluster').mean()\n",
    "print(cluster_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13da5b6-6cf1-4cd9-9ff1-06cebcc96edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Silhouette Score\n",
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(X_scaled, kmeans.labels_)\n",
    "print(f\"Silhouette Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b084d-f165-4fac-8139-975a8ccdfddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis (PCA) and visualization of K-Means clustering results\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "plt.title('PCA - KMeans Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ca701-8ca3-47f2-8cc2-fe28987dc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding articles titles to data frame\n",
    "# dodanie tytułów artykułów do data frame\n",
    "\n",
    "article_titles = [\n",
    "    \"To najgorszy ketchup, a Polacy go uwielbiają. Dietetyk ostrzega przed zakupem\", \n",
    "    \"Antycyklon Beata coraz bliżej Polski. Kolejne regiony skują się lodem [POGODA]\", \n",
    "    \"Cios dla Rosji. Armenia i USA podpisały porozumienie\", \n",
    "    \"Premier Australii żąda wyjaśnień od Rosji. 'Podejmiemy zdecydowane kroki'\", \n",
    "    \"Premier Szwecji: wyślemy trzy okręty i samolot rozpoznawczy do ochrony Bałtyku\",\n",
    "    \"Tłum ludzi szuka skarbów nad Bałtykiem. Do sieci trafiły nagrania\",\n",
    "    \"Czechy są gotowe zrezygnować z rosyjskiej ropy. 'Rosja nie może już nas szantażować'\",\n",
    "    \"Wybuch i pożar w restauracji w Czechach. Są ofiary\",\n",
    "    \"USA ograniczają eksport czipów AI. Polska krajem drugiej kategorii obok Mongolii i Nigerii\",\n",
    "    \"Kolejna wygrana i wielki rekord Djokovicia! Serb pisze historię w Melbourne\",\n",
    "    \"Arktyczna bomba uderzy w USA. Miliony ludzi zagrożone ekstremalnym mrozem\",\n",
    "    \"Hamas zaakceptował projekt zawieszenia broni\",\n",
    "    \"Skandal w Iławie. Jest reakcja komendanta wojewódzkiego policji. 'Haniebne zachowanie'\",\n",
    "    \"Sankcje USA zadziałały. Indie zamykają porty dla rosyjskich tankowców\",\n",
    "    \"Przez miesiąc jadła codziennie dwa jajka na śniadanie. Efekty były szybkie\",\n",
    "    \"Śniadanie na długowieczność. Japońska dietetyczka robi je codziennie\",\n",
    "    \"Eksperci mylili się co do jajek. Po latach prawda wyszła na jaw\",\n",
    "    \"Policja aresztowała zawieszonego prezydenta Korei Płd. Służby wdarły się do rezydencji\",\n",
    "    \"Policja w Seulu próbuje aresztować zawieszonego prezydenta. Pokonali pierwszy kordon\",\n",
    "    \"Pożary w Los Angeles. Śledczy podali prawdopodobną przyczynę\",\n",
    "    \"Coraz więcej obcokrajowców chce mieszkać w Polsce. Przodują obywatele Izraela\",\n",
    "    \"Oto najlepsze licea i technika według rankingu Perspektyw 2025\",\n",
    "    \"Rotterdam idealnym miejscem na tegoroczny city break\",\n",
    "    \"Ewenement na skalę całej Europy. Ponad 90 proc. samochodów sprzedanych w tym kraju w 2024 r. stanowiły elektryki\",\n",
    "    \"Dwaj najlepsi siatkarze świata w jednym klubie! Tomasz Fornal w duecie marzeń\",\n",
    "    \"Żołnierze z Korei Północnej przebijają w tym Rosjan. Ekspert: to nowy poziom zagrożenia\",\n",
    "    \"Węgry pozbawione prawa weta? Polskę czeka wyzwanie\",\n",
    "    \"Włochy liczą na porozumienie Izraela z Hamasem. Są gotowe wysłać żołnierzy do Strefy Gazy\",\n",
    "    \"Donald Tusk po rozmowach z Wołodymyrem Zełenskim. Mówi o 'bezwarunkowej przyjaźni'\"\n",
    "]\n",
    "emotion_df['article_title'] = article_titles\n",
    "print(emotion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38ccf1-3b82-4613-90ba-5f7622402b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning names to individual clusters, joining them to titles\n",
    "emotion_map = {0: 'pozytywne', 1: 'niepokojące', 2: 'stresujące'}\n",
    "emotion_df['Emotions'] = emotion_df['Cluster'].map(emotion_map)\n",
    "print(emotion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776430d-69e8-40b3-9f71-94b42bf3eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# presentation of final results (histogram below)\n",
    "display(emotion_df[['article_title','Emotions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f316e473-223c-4ae8-b522-d26007f79080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation with histogram\n",
    "plt.hist(emotion_df['Emotions'], bins=3, color='yellow', edgecolor='black')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.title('Rozkład emocji w artykułach')\n",
    "plt.ylabel('Liczba artykułów z dominującą daną emocją')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
